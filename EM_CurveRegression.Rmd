---
title: "EM_CurveRegression"
author: "HuanChen"
date: "2019/2/17"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(splines)
library(splines2)
library(tidyverse)
```

```{r}
## N is the number of total points on the image
N = 30

## K is the number of total points on the curve 
K = 300

## The parameterization of the curve 
w <- seq(0, 2 * pi, by = 2 * pi / K)

degree_free = 4

## Generate of the Curve
X = 0
Y = 0
Z = 0
x.para = runif(1, -1, 1)
y.para = runif(1, -1, 1)
z.para = runif(1, -1, 1)
  
x = NULL
y = NULL
z = NULL

## Basis for the spline
w <- seq(0, 2 * pi, by = 2 * pi / K)
B = bs(w, df = degree_free)
  
## Curve Generation with Spiral
coeff = runif(1, -2, 2)
X = X + x.para * w * sin(coeff * w)
Y = Y + y.para * w * cos(coeff * w)
Z = Z + z.para * w


# ## Generate the coefficients for the splines
# x.coeff = runif(degree_free, -1, 1)
# y.coeff = runif(degree_free, -1, 1)
# z.coeff = runif(degree_free, -1, 1)
# X = B %*% x.coeff
# Y = B %*% y.coeff
# Z = B %*% z.coeff


for(k in 1 : K){
    x = c(x, X[k] + rnorm(10, 0, 0.1))
    y = c(y, Y[k] + rnorm(10, 0, 0.1))
    z = c(z, Z[k] + rnorm(10, 0, 0.1))
}

plot(x,y)
```

```{r}
## PI is the matrix of p_ik
PI = matrix(1 / K, nrow = N, ncol = K)
PI_sum_old = rep(1 / K, K)

## sigma is the variance of the noise in the gaussian distribution
sigma_old = 1

## beta_x and beta_y are the coefficients for the splines to the x-axis and y-axis
beta_x_old = runif(degree_free, -10, 10)
beta_y_old = runif(degree_free, -10, 10)

likelihood_store = c()
## The procedure of the EM-Algorithm
for(t in 1 : 300){
  ## E-step
  for(i in 1 : N){
    for(k in 1 : K){
      PI[i,k] = exp(-1 / (2 * sigma_old) * ((x[i] - B[k, ] %*% beta_x_old) ^ 2 + (y[i] - B[k, ] %*%   beta_y_old)^ 2)) * PI_sum_old[k]
    }
    PI = PI / apply(PI, 1, sum)
  }

  ## M-step
  ## Update PI_sum
  PI_sum_new = 1 / N * apply(PI, 2, sum)

  ## Update sigma
  sigma_temp = 0
  for(i in 1 : N){
    for(k in 1 : K){
      sigma_temp = sigma_temp + ((x[i] - B[k, ] %*% beta_x_old)^2 + (y[i] - B[k, ] %*% beta_y_old)^2) * PI[i,k]
    }
  }
  sigma_new = sigma_temp / (2 * N)

  ## Update beta_x and beta_y
  X = c()
  Y = c()
  B_X = c()
  B_Y = c()
  for(i in 1 : N){
    for(k in 1 : K){
      X = c(X, x[i] * sqrt(PI[i, k]))
      Y = c(Y, y[i] * sqrt(PI[i, k]))
      B_X = rbind(B_X, B[k, ] * sqrt(PI[i, k]))
      B_Y = rbind(B_Y, B[k, ] * sqrt(PI[i, k]))
    }
  }
  beta_x_new  = solve(t(B_X) %*% B_X + 0.01 * diag(degree_free)) %*% t(B_X) %*% X
  beta_y_new  = solve(t(B_Y) %*% B_Y + 0.01 * diag(degree_free)) %*% t(B_Y) %*% Y
  #print(sigma)
  
## Computation of the log likelihood
likelihood = 1
for(i in 1 : N){
  likelihood_temp = 0
  for(k in 1 : K){
    likelihood_temp = likelihood_temp + PI_sum_new[k] * 1 / sigma_new ^ exp(-1/(2 *     sigma_new) ^ ((x[i] - B[k, ] %*% beta_x_new ) + (y[i] - B[k, ] %*% beta_y_new)^2 ))
  }
  likelihood = likelihood * likelihood_temp
}

PI_sum_old = PI_sum_new
sigma_old = sigma_new
beta_x_old = beta_x_new
beta_y_old = beta_y_new

print(log(likelihood))
  likelihood_store = c(likelihood_store, likelihood)
}

plot(B %*% beta_x_new, B %*% beta_y_new)
plot(x,y)
```


